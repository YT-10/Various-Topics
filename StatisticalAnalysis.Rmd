---
title: "Statistical analysis"
author: "Yusuf Tatlier"
---

## Section 1: Central Limit theorem and relevant distributions

### 1a: Distributions and simulations in R

R contains (by default) a wide variety of distributions from which can be sampled. As an example on generating samples we will look at the normal distribution.

```{r}
set.seed(1001) #Is used in order to make simulations reproducible
dnorm(0,0,1) # pdf
pnorm(0,0,1) # cdf 
qnorm(0.5)   # Inverse normal cdf
norm_sample<-rnorm(1000,0,1) # Simulating from a normal distribution with mean 0 and sd 1
```

### 1b: Determining the distribution of the data

It is essential in building a statistical model and drawing statistical inferences that the right distribution is selected for the data. Visual tools and statistical tests can be utilized in order to test assumptions regarding distributions. In this sub-section we will provide a few visual tools to test the distribution assumptions, while statistical tests will be discussed in later sections.
The following visual tools can be used in order to get an idea of the underlying distribution of the sample:

- Moment quantities (mean,sd,kurtosis,skewness)
- Kernel density plots

Data can also be explored visually in order to get a feeling of the distribution: 

- Histograms
- QQ plots
- boxplots
- empirical cumulative distribution function (ecdf)

We will use base functions for making the plots and not use packages here like ggplot2 as this is not the focus.

```{r}
set.seed(1001)
N=1000
x=rnorm(N, mean = 0, sd = 1)
```

Let's look at moment quantities of the data. By default there are no functions to determine the skewness and kurtosis, so the package 'moments' need to be loaded. 

```{r,message=FALSE}
mean(x)
sd(x)
require('moments')
skewness(x) #As the normal distribution is symmetric, the skewness of the sample
#should be close to zero
kurtosis(x) #Kurtosis of the normal distribution is 3
```

Assuming that we don't know how this data was generated, we can make a histogram and make fits for the data.

```{r}
#Set prob=TRUE, otherwise it will be a frequency histogram
hist(x,prob=TRUE,breaks=10,col="red", xlab="sample",
   main="1000 samples from standard normal distribution")
lines(density(x))
```

R contains specific packages for these type of fits as can be seen below.

```{r,message=FALSE}
require(fitdistrplus)
norm_fit<-fitdist(x,"norm")
print(norm_fit)

#The package has some plotting tools
plot(norm_fit)
```

Histograms can be a poor method for determining the shape of a distribution because it is so strongly affected by the number of bins used. Another possibility is to do a kernel density plot, which offer a much more effective way to view the shape of the distribution.
```{r}
d <- density(x)
plot(d, main="Kernel Density of sample")
polygon(d, col="red", border="blue") 
```

QQ-plots provide a way to compare the distribution of the data with a theoretical distribution. By comparing quantiles of the distributions differences in features of the two distributions can be recognized (like fatter tails or shifts). More specifically if we have a sequence of random variables $X_1,...,X_n$ and $Y_1,...,Y_n$, we can order them so that we get $X_{(1)}\leq X_{(2)}\leq...\leq X_{(n)}$ and similarly $Y_{(1)}\leq...\leq Y_{(n)}$, these are also known as order statistics. In case both datasets are generated by the same distribution, we would expect these points to lie on a line. This is what is done in QQ-plots.
In case of differences, QQ plots don't necessarily provide an answer to what the underlying distribution of the data is.
 
```{r}
set.seed(1001)
# We first determine the quantiles
y=seq(0.001,1,1/N)
y=qnorm(y)
qqplot(y,x,xlab="Theoretical quantiles",ylab="Empirical quantiles", main="QQplot")
qqline(y)
```

Boxplots provide a simple way in order to visualize quantiles and compare datasets with different labels.

```{r}
set.seed(1001)
x2=rnorm(N, mean = 0, sd = 1) #Generate data from the distribution you suspect the sample to be from
boxplot(x,x2) #Compare (original) sample to generated sample
boxplot(Sepal.Length~Species,data=iris) #Example from iris dataset
```

The ecdf, often denoted as $\hat{F_n}(t)$, basically provides a cumulative distribution function based on the data, $\hat{F_n}(t)=\frac{1}{n}\sum_{i=1}^{n}1_{X_i \leq t}$. 

```{r}
plot(ecdf(x),main="ecdf normal data")
```

## 1c: Central Limit Theorem and distribution of the mean and variance 

By the central limit theorem we know that for iid random variables $X_1,...,X_n$ with mean $\mu$ and standard deviation $\sigma$, the quantity $\bar{X_n}=\frac{1}{n} \sum_{i=1}^{n} X_{i}$ converges in distribution to a normal distribution with mean $\mu$ and standard deviation $\frac{\sigma}{\sqrt{n}}$. The quantity $\frac{\sigma}{\sqrt{n}}$ is called the standard error and goes to zero as the sample size goes to infinity, this means that the quantity $\frac{1}{n} \sum_{i=1}^{n} X_{i}$ is an unbiased and consistent estimator for $\mu$. In the last sub-section we had looked at the sample mean and it could be seen that it converges to a normal distribution. Below it will be shown that the sample variance is decreasing with an increase in the sample size.

The sample variance is given by $\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X_n})^2$. It holds that $(n-1)\times \frac{S_n^2}{\sigma^2}$ has a chi-square distribution with $n-1$ degrees of freedom.

```{r}
nsim=1000
nrep=1000
mu=1.2
sigma=3.5

set.seed(1001)
x=rnorm(nsim,mu,sigma)

sample_mean=mean(x)
SSD=sd(x)
SSE=SSD/sqrt(N)
cat("The sample mean is given by: ",sample_mean)
cat("The sample standard deviation is given by: ",SSD)
cat("The sample standard error is given by: ",SSE)
```
The SSD will converge to 3.5 with an increase in sample size, while the SSE will go to zero. This is because, as the population sample increases, the estimate of the population mean becomes more reliable.
We can also look at the distribution of the sample variance, which is done below. Notice that it coverges in distribution to a chi square distribution.

```{r}
x=replicate(nrep,rnorm(nsim,mu,sigma))

sample_var_fun=function(z){
  sample_mean=mean(z)
  sample_var<-(1/(sigma^2))*(z-sample_mean)^2
  return(sum(sample_var))
}

sample_var=apply(x,2,sample_var_fun)
chisq_sample<-rchisq(1000,999)

hist(sample_var,breaks=20,main='Sample variance multiplied by (N-1)/sigma^2',freq=FALSE) 
lines(density(sample_var),col='red',lty=1)
lines(density(chisq_sample),col='blue',lty=2)
legend("topleft", legend=c("sample", "chisq(999)"),col=c("red", "blue"), lty=1:2, cex=0.8)
```

##1d: t-distribution 

In case $\sigma$ is unknown, the (unbiased) sample standard deviation $S_n$ can be used for the distribution of the mean, in which case the resulting statistic $\frac{\bar{X_n}-\mu}{\frac{S_n}{\sqrt{n}}}$ has a t-distribution with $n-1$ degrees of freedom. The advantage of the t-test is that it has fatter tails accounting for the uncertainty of the small sample size, it converges to the normal distribution as the sample size increases however. 

```{r}
set.seed(1001)
t=rt(1000,5)
hist(t,breaks=40,ylim=c(0,0.40),freq=FALSE,,main="sample t-distribution with 5 degrees of freedom")
lines(density(t),col="red")
```

##1e: Chi squared distribution 

If $X_1,X_2,...,X_n$ have a standard normal distribution, then the quantity $Y=\sum_{i=1}^{n}X_i^{2}$ has a Chi-square distribution. Note that by this definition the sample standard deviation will have a chi-square distribution, more specifically the quantity $(n-1)\frac{\sum_{i=1}^{n}(X_i-\bar{X_n})}{\sigma^2}$ will have a chi-squared distribution with $(n-1)$ degrees of freedom.

```{r}
set.seed(1001)
cs=rchisq(1000,df=5)
hist(cs,breaks=20,freq=FALSE,,main="sample chi-squared distribution with 5 degrees of freedom")
lines(density(cs),col="red")
```

##1f: F distribution 

If a random variable $Y_1$ has a chi-squared distribution with $n$ degrees of freedom and a random variable $Y_2$ has a chi-squared distribution with $m$ degrees of freedom, then the quantity $\frac{Y_1/n}{Y_2/m}$ has a F distribution with parameters $n$ and $m$.

```{r}
set.seed(1001)
fd=rf(1000,5,5)
hist(fd,breaks=40,ylim=c(0,0.6),xlim=c(0,20),freq=FALSE,,main="sample F-distribution with parameters 5 and 5")
lines(density(fd),col="red")
```

##1g: Multivariate Normal distribution

The density of a random variable $\textbf{X}$ the multivariate normal distribution has the following form: 
$$ f_{\textbf{X}}(x_1,...,x_k)= \frac{1}{\sqrt{(2\pi)^k|\Sigma|}} exp\left(-\frac{1}{2}(\textbf{X}-\mu)^T\Sigma^{-1}(\textbf{X}-\mu)\right)$$
The term before the exponential is a normalization term, with $|\Sigma|$ being the discriminant of the covariance matrix. The term $\sqrt{(\textbf{X}-\mu)^T\Sigma^{-1}(\textbf{X}-\mu))}$ that is squared in the exponential is called the Mahalanobis distance. It provides a metric from a point to $\mu$, but incorporating the dispersion imposed by the covariance matrix. Notice that it reduces to the Euclidean distance for the identity covariance matrix. This insight is important in getting a better understanding of the multivariate normal distribution.
In the below section a contour plot is made of the multivariate normal distribution using the mixtools package. The ellipse is constructed based on a 95% confidence interval.

```{r,message=FALSE}
require('mixtools')
require('MASS')

#Making a contour plot using the sampling function within the mixtools package
set.seed(1001)
Mu=c(1,1)
Sigma=matrix(c(1,0.6,0.6,1.0),2,2)
mvn<-mvrnorm(1000, mu=Mu, Sigma, tol = 1e-6, empirical = FALSE, EISPACK = FALSE)
#The distribution is elliptic viewed from above/as a contour
plot(mvn)
ellipse(mu=Mu, sigma=Sigma, alpha = .05, npoints = 100, col="red") 
```

We can additionally make a (scatter) plot of the multivariate normal density. Note that this density is not normalized.  

```{r,message=FALSE,warning=FALSE}
require('plot3D')

#We can make a (scatter) plot of the multivariate normal density
set.seed(1001)
x1<-runif(20000,-10,10)
y1<-runif(20000,-10,10)
z1<-cbind(x1,y1)
prep_matrix<-(z1-Mu)%*%solve(Sigma)
md2<-diag(prep_matrix%*%t((z1-Mu)))
scatter3D(x1,y1,exp(-md2))
```

By performing a spectral decomposition of the covariance matrix we can show how the ellipse shape of the contour arises. First we have to show how the decomposition behaves under inversions of matrices. Defining  $U=U_{*}^{T},\Lambda=\Lambda^{-1}_{*}$ it can be shown that a positive definite matrix can be written as follows (here $U$ is the orthonormal matrix of eigenvectors and $\Lambda$ is the diagonal matrix of eigenvalues):

$$ \Sigma = U_{*}^T \Lambda_{*} U_{*} \rightarrow \Sigma^{-1}=U^{T}_{*} \Lambda_{*}^{-1} U_{*}=U^{T} \Lambda U$$
Now given a matrix of coordinates $z$ it can be written that:

$$  z^T \Sigma z= z^T U \Lambda U^{T}   \rightarrow \begin{bmatrix}
           u_{1} & u_{2}
         \end{bmatrix} \begin{bmatrix}
            \lambda_{1} & 0 \\
           0 & \lambda_{2}
         \end{bmatrix}
         \begin{bmatrix}
           u_{1}\\
           u_{2}
         \end{bmatrix} z = 1 \rightarrow \frac{(u_1^Tx)^2}{(\frac{1}{\sqrt{\lambda_1^2}})^2}+\frac{(u_2^Tx)^2}{(\frac{1}{\sqrt{\lambda_2^2}})^2} = 1 $$
         
Now let's apply this on the earlier multivariate normal distribution we had sampled from:

```{r}
nps<-200
theta=seq(0,2*pi,2*pi/(nps-1))
x=sin(theta)
y=cos(theta)
circle<-cbind(x,y)

q <- sqrt(qchisq(0.95, df = 2))

#Perform an eigen decomposition
U <- eigen(Sigma)$vectors
Lambda<- diag(eigen(Sigma)$values) 

eig_decom <- U %*% sqrt(Lambda) %*% t(U)
ellipse <-  q*(circle %*% eig_decom)
##spec_decom <- U %*% Lambda %*% t(U)
#ellipse <-  q * (circle %*% solve(Sigma))
x_points <- ellipse[,1] + Mu[1]
y_points <- ellipse[,2] + Mu[2]

plot(mvn, pch=20, xlab="x", ylab="y",xlim=c(-2,4), ylim=c(-1.5,3.5))
points(x_points, y_points,pch=20,col='red')
points(x, y,col = "blue",pch=20)
```

For a Hermitian positive-definite matrix $A$ there exists a lower triangular matrix $L$ so that $A=LL^{*}$, where $L^{*}$ is the conjugate transpose of $L$. This composition is known as a Cholesky decomposition.
For many statistical applications it is important to generate normally distributed random variables that have pre-specified correlations of which the information is contained in a given correlation matrix. The Cholesky decomposition of the correlation matrix can be used for this purpose and the lower triangular matrix obtained in this way can be multiplied with independent normally distributed random variables in order to generate random variables with this dependence structure. For the simplest example of $n=2$ and an intended correlation of $\rho$, it will be obtained that $$L=\begin{bmatrix}
            1 & 0 \\
            \rho & \sqrt{1-\rho^2}
         \end{bmatrix}$$  
This means that for two independent and standard normally distributed random variables $X_1, X_2$ it will hold for $Z_1=X_1, Z_2=Z_1+\sqrt{1-\rho^2}X_2$ that $Cov(Z_1,Z_2)=Cov(X_1,\rho X_1+\sqrt{1-\rho^2}X_2)=\rho$, giving the required correlation structure.

```{r}
#Generation of correlated normally distributed random variables
Corr_mat=matrix(c(1.0,0.6,0.6,1.0),2,2)
chol_L=chol(Corr_mat) #Cholesky decomposition
set.seed(1001)
st_norm_sample<-cbind(rnorm(1000),rnorm(1000))
target_sample<-st_norm_sample%*%chol_L+c(1,1)

plot(mvn, pch=20, xlab="x", ylab="y",xlim=c(-2,4), ylim=c(-1.5,3.5))
points(target_sample,col="red")
```

## Section 2: Estimators

An estimator is a statistic that estimates a certain quantity of a population. An estimator can be a point estimator or an interval estimators. A 'good' estimator is expected to have a few or all of the listed properties below (here the estimator $t_n=t(x_1,....,x_n)$ is assumed to estimate a quantity $\theta$ of the data given a dataset $(x_1,....,x_n)$:

  - Consistency: $t_n$ converges to $\theta$ in probability as $n$ increases.
  - Being Unbiased: This means that $Et_n=\theta$
  - Efficiency: An estimator is more efficient than another estimator if it's variance is       smaller, i.e. estimates the quantity more accurately for the same sample size.
  - Asymptotic normality: The distribution of the estimator around $\theta$, so of $\sqrt{n}(t_n-\theta)$ converges to a normal distribution. 
  
It is good to note that there is a trade-off between efficieny and bias as will be discussed later in this tutorial.

## 2a: Likelihood function and Maximum Likelihood Estimation

The likelihood function for a parameter $\theta$ and realizations/data $x_1,...,x_n$ from a random variable $X$ is defined as $\mathcal{L}(\theta|x_1,...,x_n)=f(x_1,...,x_n|\theta)$. Note that it is a function of the parameter $\theta$ and is therefore not (necessarily) a density as it doesn't necessarily have to sum or integrate to 1. The likelihood is used in frquentist statistics in order to estimate parameters by looking at what parameter is most likely in having 'caused' the seen outcomes. The estimators that are obtained in this manner are called Maximum Likelihood Estimators (MLE). Let's look at an example.

```{r}
#Likelihood is basically a function of the distribution parameter, or more specifically 
#it is the probability density as function of the distribution parameter.
#The idea is that the parameter value that yields the highest density/likelihood is the 
#'most likely' estimator.
#'In r the 'd' prefix before a distribution gives the density value for provided values.
range=seq(0,1,0.01)
total=0
likelihood=dbinom(x=4,size=10,prob=range)
plot(range,likelihood)
which(likelihood==max(likelihood)) #leads to p=0.40 as most likely parameter
#Often the likelihood and distribution of a function get mixed up. The likelihood is a 
#function of the distribution parameter given the distribution outcome/realization.
#As a result it doesn't need to yield an integral of 1. 
sum(likelihood)/100
x_range=seq(0,10,1)
#On the other hand if we fix the probability and sum over all outcomes, this will yield 1 
for(i in 1:11){total=total+dbinom(x=x_range[i],size=10,prob=0.40)}
print(total)

#With only one realization we can of course not estimate the probability parameter very precisely
#also because the realization needs to be an integer. With multiple observations however we
#can multiply the ikelihoods in order to come to a more accurate (MLE) estimation.

set.seed(1001)
binom_sample=rbinom(n=30,size=10,prob=0.35)
sum(binom_sample)/(10*30) #Moment estimator

likelihood=rep(1,101)
for(i in 1:30){
  obs_likelihood=dbinom(x=binom_sample[i],size=10,prob=range)
  likelihood = likelihood*obs_likelihood
}
which(likelihood==max(likelihood)) #Leads to p=0.35


set.seed(1001)
poisson_sample<-rpois(100,2.44) #This provides the sample from which we want to estimate lambda

#Log-likelihood is often used as it provides the same result while it has useful properties
LL <- function(lambda) {
     R = dpois(poisson_sample, lambda)
     #
     sum(log(R)) #Aim is to minimize this quantity
}

lambda_vector<-seq(0,5,0.01)
plot(lambda_vector,sapply(lambda_vector,LL),xlab='lambda',ylab='LL',main='MLE for poisson sample')
MLE_vector<-sapply(lambda_vector,LL)
index_MLE=which(MLE_vector==max(MLE_vector))
MLE_lambda<-lambda_vector[index_MLE] #Note that the outcome is 2.46 and very close to actual lambda of 2.44 which was used.

```

## 2b: Moment estimators

The method of moment estimators relates the population moments to the quantities that need to be estimated. It is a simple method with simple assumptions that nevertheless yields consistent estimators. \\ 
Below an example is given for the normal distribution. It boils down to solving the quantities for estimation in terms of the moments, as two quantities are estimated the first two moments need to be used:
$$  EX=\mu, EX^2=\sigma^2+\mu^2 \rightarrow \mu=\bar{X_n},\sigma^2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X})^2 $$
```{r}
set.seed(1001)
sample<-rnorm(10000,mean=1.2,sd=3.5)
first_mom<-mean(sample)
mu<-first_mom
sigma<-sqrt(mean((sample-first_mom)^2))
cat("Estimated mean is:",mu)
cat("Estimated standard deviation is:",sigma)
```

## Section 3: Statistical tests

Statistical tests are used to test certain assumptions or hypotheses regarding certain statistical quantities. 

## 3a: t-tests

There are different t-tests, the most simple being the one-sample t-test which is also used in linear regression to test whether estimated factors are non-zero. Additionally two-sample t-tests exist in order to test the mean between two different samples, depending on whether the two sample groups are dependent or independent different tests apply. Below an example is given of the two-sample t-test in which the two samples are independent.

```{r}
t1<-iris[iris$Species=="setosa",1]
t2<-iris[iris$Species=="versicolor",1]
t<-t.test(t1, t2, alternative = "two.sided", var.equal = FALSE)
print(t)
print(t$p.value) 
```
Note that this test rejects the null hypothesis that the means of the Sepal Lengths of the two groups (setosa and versicolor) are equal.

## 3b: Normality tests

Different tests exist to test the normality of a sample, two examples are the Jarque-Bera and Shapiro-Wilk tests.

```{r,message=FALSE}
set.seed(1001)
sample<-rnorm(100,mean=1.2,sd=3.5)
jarque.test(sample)
shapiro.test(sample)

require(fitdistrplus)
fitdist(sample,"norm")
```

## 3c: Kolmogorov-Smirnov test

The Kolmogorov-Smirnov (KS) test is a non-parametric test in order to assess whether two samples are from the same distribution. The KS test does this by comparing the distance between the ecdf of the sample and the theoretical comparison cdf (in case of an one sample test) or the distance between ecdfs of two samples(in case of a two sample test). 
Here we will specifically look at the second case a provide an example below. More specifically in this case given samples $x_1,...,x_n$ and $y_1,..,y_m$ from two random variables $X$ and $Y$ respectively, with coresponding ecdfs $\hat{F}_{X,n}(x)$ and $\hat{F}_{Y,m}(y)$, the KS test looks at the following quantity:
$$ sup_z |\hat{F}_{X,n}(z)-\hat{F}_{Y,m}(z)|  $$
If the both samples are from the same distribution (or a very similar distribution) this metric quantity should be small. The null hypothesis for the KS test is that both samples are from the same distribution.
As can be seen above the KS test is a quite intuitive and pragmatic (non-paramateric) test.   
Below an example is given.

```{r}
#Generate samples
set.seed(1001)
sample_1<-rnorm(100,mean=0,sd=1)
set.seed(2001)
sample_2<-rnorm(100,mean=0,sd=1)
set.seed(3001)
sample_3<-rnorm(100,mean=0.5,sd=5.9)

#Load package
require("stats")
ks.test(sample_1,sample_2) #Note that based on a significance level of 0.05 we can not reject the null hypothesis that both samples are from the same distribution. Note that for larger samples the p-value will go up and we have more confidence in not rejecting the null hypothesis.
ks.test(sample_1,sample_3) #Note that the p-value is very significant and we reject the null hypothesis in this case. In this case even with a sample size of 100 we can draw such a conclusion, but in case the underlying distributions are more similar this will be more difficult and will require a larger sample size.
```

## 4: Sampling techniques

Sampling can be incredible useful in order to approximate certain quantities that are difficult to determine analytically. In this section we will look at how sampling can be performed and discuss some examples.


## 4a: Monte Carlo simulations

Monte Carlo simulation is a simulation technique in which process is repeated in order to derive characteristics from the underlying probability distribution. It is especially an interesting option if characteristics of a distribution are difficult to determine analytically.
Suppose that we have a distribution with mean $\mu$ and standard deviation $\sigma$ and we are interested in its first moment. We can sample $X_1,X_2,...,X_n$ from this distribution and use $\frac{1}{n}\sum_{i=1}^{n}X_i$ as approximation of the mean. By the Central Limit Theorem we know that this is an unbiased estimator of the mean with standard error $\sigma/\sqrt{n}$. Similarly we can also approximate other characteristics of the distribution. 


```{r}
#Ex1:Mean of a distribution and standard error
#Look at a Beta(5,3) distribution
set.seed(1001)
mean_vector<-c()
an_mean<-5/(5+3)
an_var<-(5*3)/((5+3)^2*(5+3+1))

for(i in 1:100){
  r_beta<-rbeta(100,5,3)
  sample_mean<-mean(r_beta)
  mean_vector<-c(mean_vector,sample_mean)
}

cat("Standard deviation in the vector is:",sd(mean_vector))
cat("Theoretical standard error is:",sqrt(an_var)/sqrt(100))

#Ex2: We want to obtain P(X>1) for X standard normal
set.seed(1001)
sample=rnorm(10000)
prob=1-pnorm(1)
mc_est=sum(sample>1)/length(sample)
# Note that the mc technique gives a very accurate outcome

#Now look at the outcome for different sample sizes
mc_prob_est=function(sample_size,value){
  sample=rnorm(sample_size)
  mc_est=sum(sample>1)/sample_size
  return(mc_est)
}

iter_seq=seq(0,10000,20)
mc_vector=c()

for(i in iter_seq){
  mc_vector=c(mc_vector,mc_prob_est(i,1))
}

plot(iter_seq,mc_vector,xlab="number of simulations",ylab="mc_estimate",main="mc_estimate P(X>1) for different number of simulations")
abline(h=prob,col="red")

#Construct a confidence interval
range_up<-prob+1/sqrt(iter_seq)
range_down<-prob-1/sqrt(iter_seq)
lines(iter_seq,range_up,col="green")
lines(iter_seq,range_down,col="green")
```

### 4b: Inverse sampling method

The inverse sampling method can be used to sample from complicated distributions, which also might not be directly available in R. It uses that for a random variable with an uniform distribution $U$ and the normal cdf $F_X$, the random variable $F_X^{-1}(U)$ has a normal distribution.

```{r}
unif_samples<-runif(1000)
normal_samples<-qnorm(unif_samples)
hist(normal_samples,freq=FALSE)
```

Now let's look at a somewhat more complicated distribution that has the following probability density: 
$$   \frac{\Gamma(7)}{\Gamma(5)}\times  x \times (1-x)^4$$
This is actually a special case of a beta distribution with $\alpha=2$ and $\beta=5$.
Here the gamma function for $n$ reduces to the (ordinary) factorial for $n-1$ so that $\Gamma(7)=6!=720$ and $\Gamma(5)=4!=24$, so that the ratio becomes 30. Let's first show that this represents a proper distribution, i.e. integrates to 1 on [0,1].

```{r}
beta_pdf<-function(x){
  30*x*(1-x)^4
}

integrate(beta_pdf,0,1)
x<-seq(0,1,0.001)
y<-beta_pdf(x)

hist(rbeta(10000,2,5),freq=FALSE,main="Beta distribution with alpha=2 and beta=5",xlab='x',ylab='pdf')
lines(x,y,col='red')
```

Now we show that the inverse sampling method yields the same distribution. We first need the cdf which we can obtain by integrating the pdf, which is straightforward as this is a polynomial (here we denote the Beta(2,5) random variable by $X$):
$$ F_X(x)=30 \times \left(\frac{x^6}{6}-\frac{4x^5}{5}+\frac{3x^4}{2}-\frac{4x^3}{3}+\frac{x^2}{2}\right)$$
We now basically have to sample from the (standard) uniform distribution and invert these using the above cdf function.

```{r}
beta_cdf<-function(x){
  30*((x^6/6)-(4*(x^5)/5)+(3*(x^4)/2)-(4*(x^3)/3)+((x^2)/2))
}

diff_fun<-function(x1,y1){
  y1-beta_cdf(x1)
}

temp_fun<-function(y2){
  uniroot(diff_fun,y1=y2,c(0,1))$root
}

z<-runif(10000)
temp<-sapply(z,temp_fun)
hist(temp,freq=FALSE,xlab='x',ylab='pdf',main='Histogram for inverse sampling method')
lines(x,y,col='red')

```

## 4c: Rejection sampling

Rejection sampling is a method in order to get samples from a random variable $X$ with density $f_X(x)$ using samples from a random variable $Y$ with density function $g_Y(y)$. We sample from $Y$ and accept the sample $y$ with probability $\frac{f_X(y)}{Mg_Y(y)}$. Here $M$ is an upper bound for $\frac{f_X}{g_Y}$ over the entire support of both distributions but ideally as small as possible for the efficiency of the algorithm.
The reason that rejection sampling works is that the probability of acceptance is high at locations where the density is high, as a result points are 'proportionally' selected.

```{r}
#We want to sample from a beta(5,3) distribution using a standard normal distribution
set.seed(1001)
n_sim=100000

norm_vals=rnorm(n_sim)
acc_vals=c()
M=max(dbeta(norm_vals,5,3)/dnorm(norm_vals))

for(i in 1:n_sim){
  if(dbeta(norm_vals[i],5,3)/(M*dnorm(norm_vals[i]))>runif(1)){
    acc_vals=c(acc_vals,norm_vals[i])
  }  
}

ref_dist<-rbeta(n_sim,5,3)
hist(acc_vals,breaks=100,freq=FALSE)
lines(density(ref_dist),col="red")

```

##4d: Monte Carlo Markov Chain methods (MCMC)

MCMC methods are specific type of Monte Carlo methods that use a markov chain process, with transition rates $p(x,y)$ between any two states $x$ and $y$, that converges to the desired (stationary) distribution $\pi(x)$. One can sample from the desired distribution by first letting the markov chain run for some time. 

Metropolis-Hastings is a Monte Carlo Markov Chain (MCMC) method, that uses the accept-reject approach. It uses that for the stationary distribution it will hold that $\pi(x)p(x,y)=\pi(y)p(y,x)$. Instead of the transition probabilities $p(x,y)$ we now use the transition probabilities $q(x,y)$ corresponding to another target distribution, which will be accepted with a probability $\alpha(x,y)$ and rejected otherwise, similar to the earlier rejection sampling technique. Below we will implement and demonstrate this technique.

```{r}

#Ex. We want to sample from a Beta(5,3) distribution using a standard normal distribution as for the transitions.
x=0.5
xt_vec=c()

for(t in 1:100000){
  norm_sample=rnorm(1)
  Qxy=norm_sample
  y=x+Qxy

#It is important to note that due to the symmetric target distribution it will mean that Qxy=Qyx, i.e. probability of a transition from x to y is the same as the probability of the opposite transition. As a result the ratio Qxy/Qyx falls out.
  A=min(1,dbeta(y,5,3)/dbeta(x,5,3))
  unif=runif(1)
  if(A>unif){
    x=y
  }
  else{
    x=x
  }
  xt_vec=c(xt_vec,x)
}

hist(xt_vec,breaks=100,freq=FALSE,main="Beta(5,3) sampling with Metropolis-Hastings")
lines(density(rbeta(100000,5,3)),col="red")
```

Gibbs sampling is a method that can be used to obtain samples from a joint distribution, by sampling from the marginal conditional distributions. More specifically the following steps are followed in order to obtain samples from a joint distribution $(X_1,X_2,...,X_n)$:

1. Start with some value $(x^{(0)}_1,x^{(0)}_2,...,x^{(0)}_n)$

2. Obtain the first sample for the joint distribution by sampling once from all the $n$ conditional distributions $(X^{(1)}_j|X^{(i)}_1,...,X^{(i)}_{j-1},X^{(i)}_{j+1},...,X^{(i)}_{n})$

3. Repeat this process $m$ times in order to obtain a total of $m$ samples.
Note that a total of $n \times m$ individual sampling operations are needed. Below we give a (standard) example of sampling in a bivariate normal distribution.

```{r,message=FALSE}
require('car')

xy=c(-1,1)
mu_x=2;mu_y=3
sd_x=2;sd_y=3
rho_xy=0.4
sample_len=10000

sample_mat=matrix(,sample_len,2)

for(i in 1:sample_len){
  x=rnorm(1,mu_x+rho_xy*(sd_y/sd_x)*(y-mu_y),(1-rho_xy^2)*sd_x^2)
  y=rnorm(1,mu_y+rho_xy*(sd_y/sd_x)*(x-mu_x),(1-rho_xy^2)*sd_y^2)
  xy=c(x,y)
  sample_mat[i,]=xy
}

#Plot scatter with 95% confidence ellipse
plot(sample_mat,xlab="x samples",ylab="y samples",main="Joint Gibbs sample bivariate normal distribution")
dataEllipse(sample_mat[,1],sample_mat[,2],levels=c(0.95))
```

### 4f: Bootstrapping

Bootstrapping is a powerful statistical technique in which resampling is used from an empirical distribution to estimate quantities of the underlying distribution. The idea is that if the underlying distribution of a sample is unknown, the next best thing is to sample from the empirical distribution function with replacement.

```{r}
#Source: Brian Caffo
set.seed(1002)
dice_vec<-c(1:6) 
sample_vec<-sample(dice_vec,20,replace=TRUE,prob=c(1/12,1/12,1/3,1/6,1/6,1/6)) #Representing a sample size of an unfair dice
hist(sample_vec+0.001,right=TRUE,xlab='side',ylab='relative frequency',main='biased dice sample')

nboots<-1000
sample_size<-20
bootstrap_vector<-sample(sample_vec,nboots*sample_size,replace=TRUE)
resamples<-matrix(bootstrap_vector,nboots,sample_size)
medians<-apply(resamples,1,median)
sd(medians)
```

## 5: Bayesian statistics

Bayesian statistics sees probabilities as a degree of belief of the occurrence of an event, rather than as as the relative frequency of an event in case experiment would be repeated an infinite times. Bayesian statistics therefore assigns a-priori distributions to parameters rather than assuming single fixed values, which allows one to include perceived information on the parameter. Tha a-priori distribution is updated for the information in the data, in the form of the likelihood function, in order to come to an a-posteriori distribution. More specific the following relationship holds $p(\theta|X) \propto p(\theta) p(X|\theta)$. This relationship gives that the a-posteriori distribution $p(\theta|X)$ of the parameter $\theta$ given data $X$ is proportional to the prior distribution $p(\theta)$ of $\theta$ times the likelihood function $p(X|\theta)$. The right hand side needs to be divided by a normalization constant $p(X)$ in order to provide equality, but this term will not be directly relevant in all cases.
The following terminology for the types of priors is useful:
- conjugate priors: In some cases the a-posteriori distribution is from the same family of distributions as the prior distribution. In such a case the prior is called a conjugate prior.
- Informative priors: Some prior choices express specific information about the prior and are known as informative priors. An example is a narrow distribution around a certain point. In case of non-informative priors the dispersion will be high in the chosen prior.

```{r}
#Come up with sample with binomial parameter p=0.35
set.seed(1001)
binom_sample=rbinom(n=30,size=10,prob=0.35)

#Bayesian inference example with only one observation
#We choose a prior and update it with information in the data (likelihood) in order to come
#to a posterior.
range=seq(0,1,0.01)
#Beta prior choice with mean 5/(5+3)
prior=dbeta(range,5,3)
plot(range,prior,ylim=c(0,4),type="l",main="Bayesian inference example")

likelihood=dbinom(x=binom_sample[27],size=10,prob=range)
posterior=likelihood*prior

likeli_factor=100/sum(likelihood)
norm_factor=100/sum(posterior)

#Note that the posterior moves in the direction of the likelihood
lines(range,likeli_factor*likelihood,col="red")
lines(range,norm_factor*posterior,col="green")
legend("topleft", legend = c("Prior", "Likelihood","Posterior"),text.col = 1:3)

#Let's now iterate over all observations
for(i in 1:30){
  likelihood=dbinom(x=binom_sample[i],size=10,prob=range)
  posterior=likelihood*prior
  prior=posterior
}

plot(range,posterior,type="l",main="posterior including all observations")
which(posterior==max(posterior)) #leads to p=0.36 after 30 observations
```

## Main References

- Stat Online, Penn State Eberly College of Science
- Data Science Specialization in R, John Hopkins University
- Introducing Monte Carlo Methods in R, C Robert and G Casella, 2009
- Applied Staitistics with R, D Dalpiaz, 2020
- Various packages and corresponding documentation listed used in the above analyses (fitdistrplus, stats, mixtools, MASS etc)