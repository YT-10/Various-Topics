---
title: "Dependence, correlation and copulas"
author: "Yusuf Tatlier"
output: html_document
---

## Dependence and correlation

The dependence structure of a set of random variables is given by their joint distribution.
Although the (Person) correlation coefficient gives information about the dependence structure, it is only one dependence measure and does not fully capture the dependence structure of random variables. Correlation is a natural measure in the context of elliptical distributions, but can be inappropriate to use in other cases. Depending on the context the correlation should therefore be interpreted differently. As an example only for the multivariate normal distribution does a correlation of zero imply independence.

It is further good to note that the Pearson linear correlation is not invariant under non-linear transformations, as a result the situation can arise that the full correlation 1 can not be attained even if variables are fully dependent. Below an example is given.

```{r}
X=rnorm(100)
Y=2*X+4 #Linear relationship
paste("Correlation coefficient between Y and X is:",cor(Y,X))
Y=exp(X)
paste("Correlation coefficient between Y and X is:",round(cor(Y,X),2))
```

## Copulas

A copula is a function $C:[0,1]^d \rightarrow [0,1]$ for which it holds that there is a vector $(U_1,U_2,...,U_d)$ of uniform random variables so that 
$$ C(u_1,u_2,...,u_d)=P(U_1 \leq u_1,...,U_d \leq u_d)$$
Using that $Y=F_X(X) \sim U$ for an uniform random variable $U$, we can write $(U_1,...,U_d)=(F_1(X_1),...,F_d(X_d))$. Plugging it in the above expression and writing $F_i=F_{X_i}$ gives:
$$ C(u_1,u_2,...,u_d)=P(F_1(X_1) \leq u_1,...,F_d(X_d) \leq u_d)=P(X_1 \leq F^{-1}_{1}(u_1),...,X_d \leq F^{-1}_{d}(u_d)) \\
=F(F^{-1}_{1}(u_1),...,F^{-1}_{d}(u_d)) $$
Or reversely by writing $x_i=F^{-1}_{i}(u_i)$, we can obtain:
$$ C(F_1(x_1),...,F_d(x_d))=F(x_1,...,x_d) $$
This result is relates to the Sklar's theorem which states that every multivariate cumulative distribution function can be expressed in terms of its marginals. It is what makes copulas so useful as a joint distribution can be expressed in terms of it marginals through the use of copulas.
Note that it holds that $C(u_1,..,0,..,u_d)=0$ if any $u_i=0$ and $C(1,..,u_i,..,1)=u_i$ if all $u_j=1$ for $i \neq j$.

In the context of correlations a value is obtained in the range $[-1,1]$, also in the context of copulas the notion of perfect positive or negative correlation exists which is denoted by comonotonicity and countermonotonicity respectively. It can be shown that all copulas $C$ are bounded by the copulas $W_d, M_d$ which are also known as the Frechet-Hoeffding bounds:
$$ W_d(u_1,...,u_d)=min\{u_1,...,u_d\} \leq C(u_1,...,u_d) \leq max\{u_1+...+u_d-(d-1)\}= M_d(u_1,...,u_d)$$
Logically $W_d$ and $M_d$ are also known as the countermonotonicity and monotonicity copulas respecitvely. In case of $U_1,...,U_n=U$ comononotonicty holds, for the case that $d=2$ it is easy to see that $U_2=1-U_1$ gives the most countermonotonistic case. Somewhere in between lies the 'independence copula' or the 'product copula' $\Pi_d(u_1,...,u_d)=u_1u_2...u_d$.

```{r}
set.seed(1001)
unif_sample_1<-runif(1000)
unif_sample_2<-runif(1000)

M_cop<-apply(cbind(unif_sample_1,unif_sample_2),1,min)
M_cop_df<-data.frame(unif_sample_1,unif_sample_2,M_cop)

#scatterplot of the upper bound copula
require("scatterplot3d")
RGB_<-rgb(unif_sample_1,unif_sample_2,M_cop)
scatterplot3d(unif_sample_1,unif_sample_2,M_cop,angle=140,color=RGB_,M_cop_df$M_cop)

W_cop<-pmax(unif_sample_1+unif_sample_2-1,0)
prod_cop<-unif_sample_1*unif_sample_2

#Check on the bounds
mean(prod_cop<M_cop)
mean(prod_cop>W_cop)
```

## Example on convenience of copulas

For some multivariate distributions it is straightforward to set a correlation structure, the most clear example is the multivariate normal distribution. However in the more complicated situation of a multivariate distribution whose marginals follow different distributions this is not so clear. Copulas are convenient as they provide a way to do this, below a simple example is given. 

```{r}
require(MASS)
Mu=c(0,0,0)
Sigma=matrix(c(1,0.25,0.6,0.25,1,0.8,0.6,0.8,1),3,3)
mvn_sample<-mvrnorm(10000,Mu,Sigma)
cor(mvn_sample)
uni_sample<-pnorm(mvn_sample)
#Notice that the correlations are (roughly) preserved
cor(uni_sample)

mult_dist_x<-qexp(uni_sample[,1],3)
mult_dist_y<-qbeta(uni_sample[,2],2,2)
mult_dist_z<-mvn_sample[,3]
mult_dist<-cbind(mult_dist_x,mult_dist_y,mult_dist_z)
pairs(mult_dist)
#Notice that the correlation structure is perserved
cor(mult_dist)
```

##Elliptical and Archimedean copulas

As can be seen from the definition of a copula, various constructions can be set up that satisfy this definition. As a result there are also quite a lot of copulas that can be used in order to capture the dependence structure between random variables. Here two categories will be looked at, being the elliptical and Archimedean class of copulas.

Elliptical copulas are copulas for elliptical distributions, here the dependence structure will follow elliptical contours. Two simple exampes are the Gaussian copula and t-copula.

```{r}
require(copula)

norm_cop_1<-ellipCopula(family="normal",dim=2,param=-0.6)
norm_cop_2<-ellipCopula(family="normal",dim=2,param=0.85)

mvn_1<-rCopula(1000,norm_cop_1)
mvn_2<-rCopula(1000,norm_cop_2)

par(mfrow=c(1,2))
plot(mvn_1,main="Bivariate normal rho=-0.6")
plot(mvn_2,main="Bivariate normal rho=0.85")
```

The t-copula will be more spread out and will follow a star shaped form that converges to the Gaussian copula when the degrees of freedom becomes sufficiently large. 

```{r}
t_cop <- tCopula(c(0.85),dim = 2,df = 2)
ts<-rCopula(1000,t_cop)
plot(ts,main="Bivariate t")
```

Archimedean copulas allow for non-elliptic distributions, while requiring only one parameter (regardless of the dimensions) for the dependence structure. They are flexible while providing a closed form expression. Below three different Archimedean copulas are displayed.
Note that a larger parameter value leads to a higher concentration in the dependence structure.

```{r}
par(mfrow=c(3,1))

# Clayton copula is asymmetric and displays a less strong dependence for larger values

clayton_1 <- claytonCopula(2, dim = 2)
clayton_2 <- claytonCopula(10, dim = 2)
C_1 <- rCopula(500, copula = clayton_1)
C_2 <- rCopula(500, copula = clayton_2)
plot(C_1,main="Clayton copula",xlab="x",ylab="y")
points(C_2,col="red")
legend("topleft", legend=c("phi=2", "phi=10"),
       col=c("black", "red"), lty=1, cex=0.8)

# Gumbel copula is asymmetric and displays a stronger dependence for larger values, in that sense it shows opposite behavior to a Clayton copula.

gumbel_1 <- gumbelCopula(2, dim = 2)
gumbel_2 <- gumbelCopula(10, dim = 2)
G_1 <- rCopula(500, copula = gumbel_1)
G_2 <- rCopula(500, copula = gumbel_2)
plot(G_1,main="Gumbel copula",xlab="x",ylab="y")
points(G_2,col="red")
legend("topleft", legend=c("phi=2", "phi=10"),
       col=c("black", "red"), lty=1, cex=0.8)

# Frank copula is symmetric

frank_1 <- frankCopula(2, dim = 2)
frank_2 <- frankCopula(10, dim = 2)
F_1 <- rCopula(500, copula = frank_1)
F_2 <- rCopula(500, copula = frank_2)
plot(F_1,main="Frank copula",xlab="x",ylab="y")
points(F_2,col="red")
legend("topleft", legend=c("phi=2", "phi=10"),
       col=c("black", "red"), lty=1, cex=0.8)

```

## Fitting a copula

```{r,message=FALSE}
#We first simulate from a known copula
#Based on some exploration it is seen that a sample of n=50000 gives accurate outcomes.
require("VineCopula")
set.seed(1001)
cop_data <- rCopula(50000, copula = clayton_1)

#Fitting copula
fit_cop<-BiCopSelect(pobs(cop_data)[,1],pobs(cop_data)[,2],familyset=NA)
print(fit_cop)
```

## Main References

- Correlation and dependency in Risk Management: Properties and pitfalls, P Embrechts et al. 1998
- An introduction to copulas, M Haugh, 2016
- 'Copula' package in R (version 0.999-20)